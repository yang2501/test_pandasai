{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to run \"pip install -r requirements.txt\" to install dependencies\n",
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity import ClientSecretCredential\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import time\n",
    "from pandasai.smart_dataframe import SmartDataframe\n",
    "from pandasai import Agent\n",
    "import pandas as pd\n",
    "from langchain_openai.chat_models import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AzureOpenAISetup:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.tenant_id = os.environ.get(\"tenant_id\")\n",
    "        self.client_id = os.environ.get(\"client_id\")\n",
    "        self.client_secret = os.environ.get(\"client_secret\")\n",
    "        self.refresh_token()\n",
    "\n",
    "    def refresh_token(self):\n",
    "        credential = ClientSecretCredential(\n",
    "            self.tenant_id, self.client_id, self.client_secret)\n",
    "        self.token = credential.get_token(\n",
    "            \"https://cognitiveservices.azure.com/.default\")\n",
    "        os.environ[\"OPENAI_API_TYPE\"] = \"azure_ad\"\n",
    "        os.environ[\"OPENAI_API_KEY\"] = self.token.token\n",
    "        os.environ[\"AZURE_OPENAI_AD_TOKEN\"] = self.token.token\n",
    "        os.environ[\"OPENAI_API_VERSION\"] = \"2023-07-01-preview\"\n",
    "\n",
    "        self.embeddings = AzureOpenAIEmbeddings(\n",
    "            model=\"text-embedding-ada-002\",\n",
    "            openai_api_key=self.token.token,\n",
    "            azure_endpoint=\"https://do-openai-instance.openai.azure.com/\",\n",
    "        )\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        return self.embeddings\n",
    "\n",
    "    def get_token(self):\n",
    "        return self.token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = 'gpt-4o'\n",
    "azure_endpoint=\"https://do-openai-instance.openai.azure.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_setup = AzureOpenAISetup()\n",
    "# refresh token and update corresponding envs\n",
    "# call this refresh_token if needed\n",
    "azure_setup.refresh_token()\n",
    "\n",
    "# create llm from Azure OpenAI\n",
    "llm = AzureChatOpenAI(\n",
    "    streaming=False,\n",
    "    deployment_name=llm_model_name,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure PandasAI\n",
    "# see config section at \"https://docs.pandas-ai.com/getting-started\" for available options\n",
    "config = {\n",
    "    \"llm\": llm,\n",
    "    # Other configuration options as needed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrame\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"./data/standardized_analysis_ready_df.csv\")\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "# unique_visits_df = df['VISIT'].drop_duplicates()\n",
    "# print(unique_visits_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"(true | false)\"\n",
    "from pandasai.ee.vectorstores import ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the vector store\n",
    "vector_store = ChromaDB()\n",
    "description = \"You're a data engineer. You're very good at processing and transforming data before calling the custom skills.\"\n",
    "config={\"llm\": llm}\n",
    "\n",
    "description = \"You're a data analyst.\"\n",
    "\n",
    "agent = Agent(df, vectorstore=vector_store, config=config, description=description)\n",
    "agent.chat('How many devices are there?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai.skills import skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Plot bland_altman to compare device/ endpoint alignment\n",
    "\n",
    "@skill\n",
    "def bland_altman_plot(df, endpoint1, endpoint2, device1=None, device2=None, bySeverityCategory=False):\n",
    "    \"\"\"\n",
    "    Generates a Bland-Altman plot to compare two devices or two endpoints, optionally by severity category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The original DataFrame containing the data.\n",
    "            'VISIT': Visit name (e.g. VISIT2)\n",
    "            'USUBJID': Unique subject ID\n",
    "            'digital_EP': Endpoint name (e.g. WASO, AHI, etc.)\n",
    "            'digital_EP_value': Endpoint value (some numeric value)\n",
    "            'digital_EP_severity_category': Severity category of the endpoint\n",
    "            'COHORT': Treatment group (e.g. Placebo/ Treatment)\n",
    "            'DEVICE': Device name (e.g. WatchPAT, PSG, etc.)\n",
    "    endpoint1 : str\n",
    "        The first endpoint to compare (e.g., WASO).\n",
    "    endpoint2 : str\n",
    "        The second endpoint to compare (e.g., AHI).\n",
    "    device1 : str, optional\n",
    "        The first device to compare (e.g., WatchPAT).\n",
    "    device2 : str, optional\n",
    "        The second device to compare (e.g., PSG).\n",
    "    bySeverityCategory : bool, optional\n",
    "        Whether to plot the Bland-Altman plots by severity category.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    str\n",
    "        Confirmation message after plotting.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "\n",
    "    if endpoint1 == endpoint2:\n",
    "        # Filter the DataFrame for the specified endpoint and devices\n",
    "        \n",
    "        df1 = df[(df['digital_EP'] == endpoint1) & (df['DEVICE'] == device1)][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category']].rename(columns={'digital_EP_value': f'digital_EP_value_{device1}'})\n",
    "        df2 = df[(df['digital_EP'] == endpoint1) & (df['DEVICE'] == device2)][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category']].rename(columns={'digital_EP_value': f'digital_EP_value_{device2}'})\n",
    "    else:\n",
    "        # Filter the DataFrame for the specified endpoints\n",
    "        df1 = df[(df['digital_EP'] == endpoint1)][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category']].rename(columns={'digital_EP_value': f'{endpoint1}_value'})\n",
    "        df2 = df[(df['digital_EP'] == endpoint2)][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category']].rename(columns={'digital_EP_value': f'{endpoint2}_value'})\n",
    "\n",
    "    # Merge the two DataFrames on the subject ID and visit\n",
    "    df_merged = pd.merge(df1, df2, on=['USUBJID', 'VISIT', 'digital_EP_severity_category'])\n",
    "\n",
    "    # Check if there is data to plot\n",
    "    if df_merged.empty:\n",
    "        print(\"No overlapping data between the specified endpoints/devices. Please specify valid endpoints to compare. If there is one endpoint, please specify the devices to compare. You can also specify whether to plot by severity category.\")\n",
    "        return \"error\"\n",
    "\n",
    "    if bySeverityCategory:\n",
    "        # Get the unique severity categories\n",
    "        severity_categories = df_merged['digital_EP_severity_category'].unique()\n",
    "\n",
    "        # Create a plot for each severity category\n",
    "        for category in severity_categories:\n",
    "            category_data = df_merged[df_merged['digital_EP_severity_category'] == category]\n",
    "\n",
    "            # Calculate the mean and difference of the two endpoints\n",
    "            if endpoint1 == endpoint2:\n",
    "                category_data['mean'] = category_data[[f'digital_EP_value_{device1}', f'digital_EP_value_{device2}']].mean(axis=1)\n",
    "                category_data['difference'] = category_data[f'digital_EP_value_{device1}'] - category_data[f'digital_EP_value_{device2}']\n",
    "            else:\n",
    "                category_data['mean'] = category_data[[f'{endpoint1}_value', f'{endpoint2}_value']].mean(axis=1)\n",
    "                category_data['difference'] = category_data[f'{endpoint1}_value'] - category_data[f'{endpoint2}_value']\n",
    "\n",
    "            # Calculate the mean difference and limits of agreement\n",
    "            mean_diff = category_data['difference'].mean()\n",
    "            std_diff = category_data['difference'].std()\n",
    "            upper_limit = mean_diff + 1.96 * std_diff\n",
    "            lower_limit = mean_diff - 1.96 * std_diff\n",
    "            count = len(category_data['difference'])\n",
    "\n",
    "            # Create the Bland-Altman plot\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(category_data['mean'], category_data['difference'], alpha=0.5)\n",
    "            plt.axhline(mean_diff, color='gray', linestyle='solid', label=f'Mean Difference: {mean_diff:.2f}')\n",
    "            plt.axhline(upper_limit, color='red', linestyle='--', label=f'+1.96 SD: {upper_limit:.2f}')\n",
    "            plt.axhline(lower_limit, color='red', linestyle='--', label=f'-1.96 SD: {lower_limit:.2f}')\n",
    "            plt.xlabel('Means')\n",
    "            plt.ylabel(f'Difference: {endpoint1} - {endpoint2}' if endpoint1 != endpoint2 else f'Difference: {endpoint1} ({device1}) - {endpoint1} ({device2})')\n",
    "            plt.title(f'Bland-Altman Plot for {count} {endpoint1} - {endpoint2} pairs\\nSeverity Category: {category}' if endpoint1 != endpoint2 else f'Bland-Altman Plot for {count} {device1} - {device2} pairs\\nSeverity Category: {category}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        # Calculate the mean and difference of the two endpoints\n",
    "        if endpoint1 == endpoint2:\n",
    "            df_merged['mean'] = df_merged[[f'digital_EP_value_{device1}', f'digital_EP_value_{device2}']].mean(axis=1)\n",
    "            df_merged['difference'] = df_merged[f'digital_EP_value_{device1}'] - df_merged[f'digital_EP_value_{device2}']\n",
    "        else:\n",
    "            df_merged['mean'] = df_merged[[f'{endpoint1}_value', f'{endpoint2}_value']].mean(axis=1)\n",
    "            df_merged['difference'] = df_merged[f'{endpoint1}_value'] - df_merged[f'{endpoint2}_value']\n",
    "\n",
    "        # Calculate the mean difference and limits of agreement\n",
    "        mean_diff = df_merged['difference'].mean()\n",
    "        std_diff = df_merged['difference'].std()\n",
    "        upper_limit = mean_diff + 1.96 * std_diff\n",
    "        lower_limit = mean_diff - 1.96 * std_diff\n",
    "        count = len(df_merged['difference'])\n",
    "\n",
    "        # Create the Bland-Altman plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(df_merged['mean'], df_merged['difference'], alpha=0.5)\n",
    "        plt.axhline(mean_diff, color='gray', linestyle='solid', label=f'Mean Difference: {mean_diff:.2f}')\n",
    "        plt.axhline(upper_limit, color='red', linestyle='--', label=f'+1.96 SD: {upper_limit:.2f}')\n",
    "        plt.axhline(lower_limit, color='red', linestyle='--', label=f'-1.96 SD: {lower_limit:.2f}')\n",
    "        plt.xlabel('Means')\n",
    "        plt.ylabel(f'Difference: {endpoint1} - {endpoint2}' if endpoint1 != endpoint2 else f'Difference: {endpoint1} ({device1}) - {endpoint1} ({device2})')\n",
    "        plt.title(f'Bland-Altman Plot for {count} {endpoint1} - {endpoint2} pairs' if endpoint1 != endpoint2 else f'Bland-Altman Plot: for {count} {device1} - {device2} pairs')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return \"success\"\n",
    "\n",
    "# Plot change from baseline. By default, if no device is specified, it plots the change from baseline for an endpoint for all devices.\n",
    "\n",
    "@skill\n",
    "def change_from_baseline_plot(df, endpoint, device=None):\n",
    "    \"\"\"\n",
    "    Plots a change from baseline chart for different cohorts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The original DataFrame containing the data.\n",
    "            'VISIT' (e.g. VISIT3). The get_visit_number() function relies on screening visit being marked as 'Screening'. The data should be cleaned such that there are only valid visits in this column\n",
    "            'USUBJID': unique subject ID\n",
    "            'digital_EP': (e.g. WASO, AHI, etc.)\n",
    "            'digital_EP_value': (some numeric value)\n",
    "            'digital_EP_severity_category' \n",
    "            'COHORT' (e.g. Placebo/ Treatment)\n",
    "            'DEVICE' (WatchPAT, PSG, etc.)\n",
    "    endpoint : str\n",
    "        The Digital_EP to plot (e.g., WASO, AHI, etc.). If the endpoint is not provided. Ask the user a clarifying question for the endpoint.\n",
    "    device : str, optional\n",
    "        The device to plot. If not specified, the default is to make a change from baseline plot for all devices.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    def get_visit_number(visit):\n",
    "        \"\"\"\n",
    "        Extracts the numeric part of the visit name for sorting.\n",
    "        Non-numeric visits are considered invalid and return None.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        match = re.match(r'^VISIT(\\d+)$', visit, re.IGNORECASE)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "        elif visit.lower() == 'screening':\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def plot_device_data(df_filtered, endpoint, device):\n",
    "        \"\"\"\n",
    "        Helper function to plot the data for a specific device.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_filtered : DataFrame\n",
    "            Filtered DataFrame containing the data for a specific device.\n",
    "        endpoint : str\n",
    "            The Digital_EP to plot (e.g., WASO, AHI, etc.).\n",
    "        device : str\n",
    "            The device to include in the plot title.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        None\n",
    "        \"\"\"\n",
    "        \n",
    "        # Filter out invalid visits\n",
    "        df_filtered['visit_num'] = df_filtered['VISIT'].apply(get_visit_number)\n",
    "        df_filtered = df_filtered.dropna(subset=['visit_num'])\n",
    "\n",
    "        # Ensure visit_num is integer type\n",
    "        df_filtered['visit_num'] = df_filtered['visit_num'].astype(int)\n",
    "\n",
    "        # Determine the visit with the highest postfix\n",
    "        if not df_filtered['visit_num'].empty:\n",
    "            max_visit_row = df_filtered.loc[df_filtered['visit_num'].idxmax()]\n",
    "            comparison_visit = max_visit_row['VISIT']\n",
    "        else:\n",
    "            raise ValueError(\"No valid visits found.\")\n",
    "\n",
    "        # Filter the dataframe for the specified endpoint and remove rows with unknown cohort\n",
    "        df_filtered = df_filtered[df_filtered['COHORT'] != 'Unknown']\n",
    "\n",
    "        # Calculate the baseline value for each subject\n",
    "        df_baseline = df_filtered[df_filtered['VISIT'].str.lower() == 'screening'][['USUBJID', 'digital_EP_value']]\n",
    "        df_baseline = df_baseline.rename(columns={'digital_EP_value': 'digital_EP_baseline_value'})\n",
    "\n",
    "        # Merge baseline values with the original dataframe\n",
    "        df_merged = df_filtered.merge(df_baseline, on='USUBJID', how='left')\n",
    "\n",
    "        # Calculate the change from baseline for each subject at each visit\n",
    "        df_merged['calculated_change_from_baseline'] = df_merged['digital_EP_value'] - df_merged['digital_EP_baseline_value']\n",
    "\n",
    "        # Group by cohort and visit to calculate the mean, sem, and number of subjects\n",
    "        grouped = df_merged.groupby(['COHORT', 'VISIT'])\n",
    "        aggDf = grouped.agg(\n",
    "            mean=('calculated_change_from_baseline', 'mean'),\n",
    "            sem=('calculated_change_from_baseline', lambda x: x.std() / (len(x) ** 0.5)),\n",
    "            num_subjects=('calculated_change_from_baseline', 'count')\n",
    "        ).reset_index()\n",
    "\n",
    "        cohortList = aggDf['COHORT'].unique().tolist()\n",
    "\n",
    "        cohort2Color = {\n",
    "            cohortList[0]: 'black',\n",
    "            cohortList[1]: 'red'\n",
    "        }\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "        xticks = []\n",
    "        xlabels = []\n",
    "\n",
    "        for cohort in cohortList:\n",
    "\n",
    "            data = aggDf[aggDf['COHORT'] == cohort].reset_index(drop=True)\n",
    "\n",
    "            if data.empty:\n",
    "                continue\n",
    "\n",
    "            data['visit_num'] = data['VISIT'].apply(get_visit_number)\n",
    "            data = data.sort_values(by=['visit_num'])\n",
    "            visits = data['VISIT'].tolist()\n",
    "\n",
    "            x = data['visit_num'].tolist()\n",
    "            xticks += x\n",
    "            xlabels += visits\n",
    "\n",
    "            y = data['mean'].tolist()\n",
    "\n",
    "            yerr = data['sem'].tolist()\n",
    "\n",
    "            plt.errorbar(x, y, yerr=yerr, capsize=5, fmt='-o', color=cohort2Color[cohort], label=cohort, markersize=4)\n",
    "\n",
    "        xticks = list(set(xticks))\n",
    "        xticks.sort()\n",
    "\n",
    "        unique_xlabels = []\n",
    "        unique_xticks = []\n",
    "        for tick, label in zip(xticks, xlabels):\n",
    "            if tick not in unique_xticks:\n",
    "                unique_xticks.append(tick)\n",
    "                unique_xlabels.append(label)\n",
    "\n",
    "        plt.xticks(unique_xticks, unique_xlabels, rotation=45)\n",
    "        plt.legend(loc='best', prop={'size': 8})\n",
    "        title = f\"{device} {endpoint} Change from Baseline\"\n",
    "        plt.title(title, fontsize=10)\n",
    "        plt.xlabel('Visit', fontsize=10)\n",
    "        plt.ylabel(f'{endpoint} Change', fontsize=10)\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Filter by endpoint\n",
    "    df_filtered = df[df['digital_EP'] == endpoint]\n",
    "    \n",
    "    # If device is specified, plot for that device\n",
    "    if device:\n",
    "        df_filtered = df_filtered[df_filtered['DEVICE'].str.lower() == device.lower()]\n",
    "        plot_device_data(df_filtered, endpoint, device)\n",
    "    else:\n",
    "        # If no device is specified, plot for each device\n",
    "        for dev in df_filtered['DEVICE'].unique():\n",
    "            df_device = df_filtered[df_filtered['DEVICE'] == dev]\n",
    "            print(f\"Plotting for device: {dev}\")\n",
    "            plot_device_data(df_device, endpoint, dev)\n",
    "\n",
    "# Plot endpoint distribution for each device for each visit to have a general idea of device agreement\n",
    "\n",
    "@skill\n",
    "def plot_endpoint_distribution(df, endpoint, device1=None, device2=None, visit=None, bySeverityCategory=False):\n",
    "    \"\"\"\n",
    "    Plots histograms showing the distribution of a specified endpoint for each device and compares the means.\n",
    "    Optionally, plots the distribution by severity category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The original DataFrame containing the data.\n",
    "            'VISIT': Visit name (e.g. VISIT2)\n",
    "            'USUBJID': Unique subject ID\n",
    "            'digital_EP': Endpoint name (e.g. WASO, AHI, etc.)\n",
    "            'digital_EP_value': Endpoint value (some numeric value)\n",
    "            'digital_EP_severity_category': Severity category of the endpoint\n",
    "            'COHORT': Treatment group (e.g. Placebo/ Treatment)\n",
    "            'DEVICE': Device name (e.g. WatchPAT, PSG, etc.)\n",
    "    endpoint : str\n",
    "        The Digital_EP to plot (e.g., WASO, AHI, etc.).\n",
    "    device1 : str, optional\n",
    "        The first device to filter the data (e.g., WatchPAT, PSG).\n",
    "    device2 : str, optional\n",
    "        The second device to filter the data (e.g., WatchPAT, PSG).\n",
    "    visit : str, optional\n",
    "        The visit to filter the data (e.g., VISIT2). If not specified, the default is to plot for all visits where data for both devices is available.\n",
    "    bySeverityCategory : bool, optional\n",
    "        Whether to plot the distribution by severity category.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import ttest_ind\n",
    "\n",
    "    # Filter the DataFrame for the specified endpoint\n",
    "    df_filtered = df[df['digital_EP'] == endpoint]\n",
    "\n",
    "    # Filter by visit if specified\n",
    "    if visit:\n",
    "        df_filtered = df_filtered[df_filtered['VISIT'].str.lower() == visit.lower()]\n",
    "\n",
    "    # Ensure both devices are specified\n",
    "    if device1 is None or device2 is None:\n",
    "        devices = df['DEVICE'].unique()\n",
    "        if len(devices) < 2:\n",
    "            print(\"Not enough devices found in the data.\")\n",
    "            return\n",
    "        device1, device2 = devices[:2]\n",
    "\n",
    "    # Include only patients with data from both devices\n",
    "    subjects_with_both_devices = df_filtered[df_filtered['DEVICE'] == device1]['USUBJID'].isin(\n",
    "        df_filtered[df_filtered['DEVICE'] == device2]['USUBJID'])\n",
    "    common_subjects = df_filtered[df_filtered['DEVICE'] == device1][subjects_with_both_devices]['USUBJID']\n",
    "    df_filtered = df_filtered[df_filtered['USUBJID'].isin(common_subjects)]\n",
    "\n",
    "    if bySeverityCategory:\n",
    "        # Get the unique severity categories\n",
    "        severity_categories = df_filtered['digital_EP_severity_category'].unique()\n",
    "\n",
    "        # Create a plot for each severity category\n",
    "        for category in severity_categories:\n",
    "            category_data = df_filtered[df_filtered['digital_EP_severity_category'] == category]\n",
    "\n",
    "            # Create a histogram for each device within the severity category\n",
    "            plt.figure(figsize=(12, 8))\n",
    "\n",
    "            sns.histplot(data=category_data, x='digital_EP_value', hue='DEVICE', multiple='dodge', bins=20, kde=True)\n",
    "\n",
    "            subject_count = category_data['USUBJID'].nunique()\n",
    "            plt.title(f'{endpoint} Distribution by Device for Severity Category: {category} ({subject_count} subjects)')\n",
    "            plt.xlabel(f'{endpoint} Value')\n",
    "            plt.ylabel('Count')\n",
    "            plt.legend(title='Device')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        # Identify visits where data for both devices is available\n",
    "        visits = df_filtered['VISIT'].unique()\n",
    "        valid_visits = []\n",
    "        for v in visits:\n",
    "            visit_data = df_filtered[df_filtered['VISIT'].str.lower() == v.lower()]\n",
    "            if all(d in visit_data['DEVICE'].unique() for d in [device1, device2]):\n",
    "                valid_visits.append(v)\n",
    "        visits = valid_visits\n",
    "\n",
    "        statistics = []\n",
    "\n",
    "        # Create a plot for each visit\n",
    "        for visit in visits:\n",
    "            visit_data = df_filtered[df_filtered['VISIT'].str.lower() == visit.lower()]\n",
    "\n",
    "            device_data1 = visit_data[visit_data['DEVICE'].str.lower() == device1.lower()]\n",
    "            device_data2 = visit_data[visit_data['DEVICE'].str.lower() == device2.lower()]\n",
    "\n",
    "            if device_data1.empty or device_data2.empty:\n",
    "                print(f\"No data found for endpoint '{endpoint}' in visit '{visit}' for devices '{device1}' and '{device2}'.\")\n",
    "                continue\n",
    "\n",
    "            # Plot the histograms\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(20, 6))\n",
    "            axs[0].hist(device_data1['digital_EP_value'], bins=20, alpha=0.7, color='blue', edgecolor='black', label=device1)\n",
    "            axs[0].set_title(f'{endpoint} values for {device1} in {visit}', fontsize=15)\n",
    "            axs[0].set_xlabel(f'{endpoint} Value', fontsize=12)\n",
    "            axs[0].set_ylabel('Frequency', fontsize=12)\n",
    "            axs[0].grid(axis='y', alpha=0.75)\n",
    "            axs[0].legend()\n",
    "\n",
    "            axs[1].hist(device_data2['digital_EP_value'], bins=20, alpha=0.7, color='green', edgecolor='black', label=device2)\n",
    "            axs[1].set_title(f'{endpoint} values for {device2} in {visit}', fontsize=15)\n",
    "            axs[1].set_xlabel(f'{endpoint} Value', fontsize=12)\n",
    "            axs[1].set_ylabel('Frequency', fontsize=12)\n",
    "            axs[1].grid(axis='y', alpha=0.75)\n",
    "            axs[1].legend()\n",
    "\n",
    "            plt.suptitle(f'Distribution of {endpoint} values in {visit}', fontsize=18)\n",
    "\n",
    "            # Calculate the means and perform a t-test to compare means\n",
    "            mean1 = device_data1['digital_EP_value'].mean()\n",
    "            mean2 = device_data2['digital_EP_value'].mean()\n",
    "            t_stat, p_value = ttest_ind(device_data1['digital_EP_value'], device_data2['digital_EP_value'])\n",
    "\n",
    "            # Generate conclusion based on p-value\n",
    "            if p_value < 0.05:\n",
    "                conclusion = f'There is a statistically significant difference between {device1} and {device2} in {visit} (p < 0.05).'\n",
    "            else:\n",
    "                conclusion = f'There is no statistically significant difference between {device1} and {device2} in {visit} (p >= 0.05).'\n",
    "\n",
    "            # Store statistics\n",
    "            statistics.append({\n",
    "                'Visit': visit,\n",
    "                f'{device1} Mean': mean1,\n",
    "                f'{device2} Mean': mean2,\n",
    "                'p-value': p_value,\n",
    "                'Conclusion': conclusion\n",
    "            })\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "            plt.show()\n",
    "\n",
    "        # Create a DataFrame from the statistics and display it\n",
    "        stats_df = pd.DataFrame(statistics)\n",
    "        print(stats_df)\n",
    "        for conclusion in stats_df['Conclusion']:\n",
    "            print(conclusion)\n",
    "\n",
    "# Plot endpoint correlation\n",
    "\n",
    "@skill\n",
    "def plot_correlation(df, endpoint1, endpoint2, device1=None, device2=None, bySeverityCategory=False):\n",
    "    \"\"\"\n",
    "    Plots scatter plots showing the correlation between two endpoints for each device.\n",
    "    Optionally, plots the correlation by severity category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The original DataFrame containing the data.\n",
    "            'VISIT': Visit name (e.g. VISIT2)\n",
    "            'USUBJID': Unique subject ID\n",
    "            'digital_EP': Endpoint name (e.g. WASO, AHI, etc.)\n",
    "            'digital_EP_value': Endpoint value (some numeric value)\n",
    "            'digital_EP_severity_category': Severity category of the endpoint\n",
    "            'COHORT': Treatment group (e.g. Placebo/ Treatment)\n",
    "            'DEVICE': Device name (e.g. WatchPAT, PSG, etc.)\n",
    "    endpoint1 : str\n",
    "        The first endpoint to compare (e.g., WASO).\n",
    "    endpoint2 : str\n",
    "        The second endpoint to compare (e.g., AHI).\n",
    "    device1 : str, optional\n",
    "        The first device to filter the data (e.g., WatchPAT, PSG).\n",
    "    device2 : str, optional\n",
    "        The second device to filter the data (e.g., WatchPAT, PSG).\n",
    "    bySeverityCategory : bool, optional\n",
    "        Whether to plot the correlation by severity category.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    from scipy.stats import linregress, spearmanr, pearsonr\n",
    "\n",
    "    # Filter the DataFrame for the specified endpoints\n",
    "    if endpoint1 == endpoint2:\n",
    "        \n",
    "        devices = df['DEVICE'].unique()\n",
    "        device1, device2 = devices[:2]\n",
    "        \n",
    "        df1 = df[(df['digital_EP'] == endpoint1) & (df['DEVICE'] == device1)][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category', 'DEVICE']].rename(columns={'digital_EP_value': f'digital_EP_value_{device1}'})\n",
    "        df2 = df[(df['digital_EP'] == endpoint1) & (df['DEVICE'] == device2)][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category', 'DEVICE']].rename(columns={'digital_EP_value': f'digital_EP_value_{device2}'})\n",
    "    else:\n",
    "        df1 = df[df['digital_EP'] == endpoint1][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category', 'DEVICE']].rename(columns={'digital_EP_value': f'{endpoint1}_value'})\n",
    "        df2 = df[df['digital_EP'] == endpoint2][['USUBJID', 'VISIT', 'digital_EP_value', 'digital_EP_severity_category', 'DEVICE']].rename(columns={'digital_EP_value': f'{endpoint2}_value'})\n",
    "\n",
    "    # Merge the two DataFrames on the subject ID and visit\n",
    "    df_merged = pd.merge(df1, df2, on=['USUBJID', 'VISIT', 'digital_EP_severity_category'], suffixes=(f'_{device1}', f'_{device2}'))\n",
    "\n",
    "    # Check if there is data to plot\n",
    "    if df_merged.empty:\n",
    "        print(\"Please specify valid endpoints to compare. If there is one endpoint, please specify the devices to compare. You can also specify whether to plot by severity category.\")\n",
    "        return \"No overlapping data to plot.\"\n",
    "\n",
    "    def create_plot(x, y, hue, title, xlabel, ylabel):\n",
    "        # Calculate the correlation coefficients and line of best fit\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(x, y)\n",
    "        line = slope * x + intercept\n",
    "        spearman_corr, spearman_p_value = spearmanr(x, y)\n",
    "        pearson_corr, pearson_p_value = pearsonr(x, y)\n",
    "\n",
    "        # Create the scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x=x, y=y, hue=hue, palette='deep')\n",
    "        plt.plot(x, line, color='red', label=f'Line of Best Fit: y={slope:.2f}x+{intercept:.2f}')\n",
    "        plt.title(f'{title}\\n'\n",
    "                  f'Spearman Correlation: {spearman_corr:.2f}\\n       '\n",
    "                  f'Pearson Correlation: {pearson_corr:.2f}\\n p-value: {pearson_p_value:.4f}')\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if bySeverityCategory:\n",
    "        # Get the unique severity categories\n",
    "        severity_categories = df_merged['digital_EP_severity_category'].unique()\n",
    "\n",
    "        # Create a plot for each severity category\n",
    "        for category in severity_categories:\n",
    "            category_data = df_merged[df_merged['digital_EP_severity_category'] == category]\n",
    "\n",
    "            if endpoint1 == endpoint2:\n",
    "                x = category_data[f'digital_EP_value_{device1}']\n",
    "                y = category_data[f'digital_EP_value_{device2}']\n",
    "            else:\n",
    "                x = category_data[f'{endpoint1}_value']\n",
    "                y = category_data[f'{endpoint2}_value']\n",
    "\n",
    "            create_plot(x, y, category_data[f'DEVICE_{device1}'],\n",
    "                        f'{device1} {endpoint1} vs {device2} {endpoint2} Correlation\\nSeverity Category: {category}',\n",
    "                        f'{device1} {endpoint1} Value', f'{device2} {endpoint2} Value')\n",
    "    else:\n",
    "        if endpoint1 == endpoint2:\n",
    "            x = df_merged[f'digital_EP_value_{device1}']\n",
    "            y = df_merged[f'digital_EP_value_{device2}']\n",
    "        else:\n",
    "            x = df_merged[f'{endpoint1}_value']\n",
    "            y = df_merged[f'{endpoint2}_value']\n",
    "\n",
    "        create_plot(x, y, df_merged[f'DEVICE_{device1}'],\n",
    "                    f'{device1} {endpoint1} vs {device2} {endpoint2} Correlation',\n",
    "                    f'{device1} {endpoint1} Value', f'{device2} {endpoint2} Value')\n",
    "\n",
    "# Plot confusion matrix by severity category\n",
    "\n",
    "@skill\n",
    "def severity_category_confusion_matrix(df, endpoint, device=None, visit1='Screening', visit2=None):\n",
    "    \"\"\"\n",
    "    Generates confusion matrices for severity categories of an endpoint between two visits for each treatment cohort.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The original DataFrame containing the data.\n",
    "            'VISIT': Visit name (e.g. VISIT2)\n",
    "            'USUBJID': Unique subject ID\n",
    "            'digital_EP': Endpoint name (e.g. WASO, AHI, etc.)\n",
    "            'digital_EP_severity_category': Severity category of the endpoint\n",
    "            'COHORT': Treatment group (e.g. Placebo/ Treatment)\n",
    "            'DEVICE': Device name (e.g. WatchPAT, PSG, etc.)\n",
    "    endpoint : str\n",
    "        The endpoint to analyze (e.g., WASO, AHI, etc.).\n",
    "    device : str, optional\n",
    "        The device to filter the data (e.g., WatchPAT, PSG). If not specified, a random device is chosen.\n",
    "    visit1 : str, optional\n",
    "        The first visit to compare (default is 'Screening').\n",
    "    visit2 : str, optional\n",
    "        The second visit to compare (default is the latest valid visit).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    if visit2 is None:\n",
    "        # Get the latest valid visit\n",
    "        visits = df['VISIT'].str.extract(r'(\\d+)', expand=False).dropna().astype(int)\n",
    "        latest_visit_num = visits.max()\n",
    "        visit2 = f'VISIT{latest_visit_num}'\n",
    "    \n",
    "    # Filter the DataFrame for the specified endpoint\n",
    "    df_filtered = df[df['digital_EP'] == endpoint]\n",
    "    \n",
    "    if device is None:\n",
    "        device = np.random.choice(df_filtered['DEVICE'].unique())\n",
    "    \n",
    "    df_filtered = df_filtered[df_filtered['DEVICE'] == device]\n",
    "    \n",
    "    # Get the unique treatment cohorts\n",
    "    cohorts = df_filtered['COHORT'].unique()\n",
    "    \n",
    "    # Define the order of severity categories\n",
    "    severity_order = ['Severe', 'Moderate', 'Mild', 'No']\n",
    "    \n",
    "    for cohort in cohorts:\n",
    "        df_cohort = df_filtered[df_filtered['COHORT'] == cohort]\n",
    "        \n",
    "        # Filter data for visit1 and visit2\n",
    "        df_visit1 = df_cohort[df_cohort['VISIT'].str.lower() == visit1.lower()][['USUBJID', 'digital_EP_severity_category']]\n",
    "        df_visit2 = df_cohort[df_cohort['VISIT'].str.lower() == visit2.lower()][['USUBJID', 'digital_EP_severity_category']]\n",
    "        \n",
    "        # Rename columns to avoid confusion when merging\n",
    "        df_visit1 = df_visit1.rename(columns={'digital_EP_severity_category': f'severity_category_{visit1}'})\n",
    "        df_visit2 = df_visit2.rename(columns={'digital_EP_severity_category': f'severity_category_{visit2}'})\n",
    "        \n",
    "        # Merge the two visits on USUBJID\n",
    "        df_merged = pd.merge(df_visit1, df_visit2, on='USUBJID')\n",
    "        \n",
    "        if df_merged.empty:\n",
    "            print(f\"No overlapping data between {visit1} and {visit2} for cohort {cohort}.\")\n",
    "            continue\n",
    "\n",
    "        # Generate the confusion matrix\n",
    "        y_true = pd.Categorical(df_merged[f'severity_category_{visit1}'], categories=severity_order, ordered=True)\n",
    "        y_pred = pd.Categorical(df_merged[f'severity_category_{visit2}'], categories=severity_order, ordered=True)\n",
    "        \n",
    "        cm = pd.crosstab(y_true, y_pred, rownames=[f'Severity Category at {visit1}'], colnames=[f'Severity Category at {visit2}'], dropna=False)\n",
    "        \n",
    "        if cm.empty:\n",
    "            print(f\"No data to plot for cohort {cohort}.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate percentages\n",
    "        cm_percentage = cm.div(cm.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        # Create annotations with counts and percentages\n",
    "        annot = cm.astype(str) + \"\\n\" + cm_percentage.round(2).astype(str) + '%'\n",
    "\n",
    "        # Plot the confusion matrix\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=severity_order, yticklabels=severity_order, cbar=False)\n",
    "        ax = sns.heatmap(cm, annot=annot, fmt='', cmap='Blues', xticklabels=severity_order, yticklabels=severity_order, cbar=False)\n",
    "        plt.xlabel(f'Severity Category at {visit2}', )\n",
    "        plt.ylabel(f'Severity Category at {visit1}')\n",
    "        plt.title(f'Confusion Matrix of Severity Categories for {endpoint}\\nDevice: {device}, Cohort: {cohort}')\n",
    "        ax.xaxis.set_ticks_position('top')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Categorized Strip Plot\n",
    "@skill\n",
    "def categorized_strip_plot(df, endpoint, gold_standard_device, visit=None):\n",
    "    \"\"\"\n",
    "    Creates categorized strip plots for each device to visualize incorrect classifications compared to a gold standard device.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The original DataFrame containing the data.\n",
    "            'VISIT': Visit name (e.g. VISIT2)\n",
    "            'USUBJID': Unique subject ID\n",
    "            'digital_EP': Endpoint name (e.g. WASO, AHI, etc.)\n",
    "            'digital_EP_value': Endpoint value (some numeric value)\n",
    "            'digital_EP_severity_category': Severity category of the endpoint\n",
    "            'COHORT': Treatment group (e.g. Placebo/ Treatment)\n",
    "            'DEVICE': Device name (e.g. WatchPAT, PSG, etc.)\n",
    "    endpoint : str\n",
    "        The endpoint to analyze (e.g., WASO, AHI, etc.).\n",
    "    gold_standard_device : str\n",
    "        The device to be used as the gold standard for classification (e.g., PSG).\n",
    "    visit : str, optional\n",
    "        The visit to filter the data (default is to use all visits).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Define severity order\n",
    "    severity_order = ['No', 'Mild', 'Moderate', 'Severe']\n",
    "    \n",
    "    # Filter the DataFrame for the specified endpoint and visit\n",
    "    df_filtered = df[df['digital_EP'] == endpoint]\n",
    "    \n",
    "    if visit:\n",
    "        df_filtered = df_filtered[df_filtered['VISIT'].str.lower() == visit.lower()]\n",
    "\n",
    "    # Ensure device names are handled case-insensitively\n",
    "    df_filtered['DEVICE'] = df_filtered['DEVICE'].str.lower()\n",
    "    gold_standard_device = gold_standard_device.lower()\n",
    "\n",
    "    # Create a pivot table to compare devices\n",
    "    df_pivot = df_filtered.pivot_table(index='USUBJID', columns='DEVICE', values='digital_EP_severity_category', aggfunc='first')\n",
    "    \n",
    "    # Check if the gold standard device is in the DataFrame\n",
    "    if gold_standard_device not in df_pivot.columns:\n",
    "        print(f\"The gold standard device '{gold_standard_device}' is not in the DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    # Iterate over devices to create plots\n",
    "    for device in df_pivot.columns:\n",
    "        if device == gold_standard_device:\n",
    "            continue\n",
    "        \n",
    "        # Create a DataFrame for plotting\n",
    "        plot_df = df_pivot[[gold_standard_device, device]].dropna().reset_index()\n",
    "        plot_df['Severity'] = plot_df[gold_standard_device]\n",
    "        \n",
    "        # Create the strip plot for the gold standard device\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.stripplot(x=gold_standard_device, y='USUBJID', data=plot_df, order=severity_order, palette='deep', size=8, alpha=0.6, jitter=False)\n",
    "        \n",
    "        # Create the strip plot for the current device, using hue to differentiate severity categories\n",
    "        sns.stripplot(x=device, y='USUBJID', data=plot_df, order=severity_order, hue='Severity', size=6, alpha=0.6, dodge=True, palette='deep', jitter=True)\n",
    "        \n",
    "        plt.xlabel(f'True Severity Category ({gold_standard_device.upper()} {endpoint})')\n",
    "        plt.ylabel('Subjects')\n",
    "        plt.title(f'Misclassification for {device.upper()} {endpoint} compared to {gold_standard_device.upper()} {endpoint}, Visit: {visit if visit else \"All\"}')\n",
    "        plt.legend(title=f'{device.upper()} {endpoint} Severity Category', bbox_to_anchor=(1.05, 1), loc='upper left', labels=severity_order, handles=[plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=c, markersize=10) for c in sns.color_palette('deep', len(severity_order))])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "agent = Agent(df, vectorstore=vector_store, config=config, description=description)\n",
    "agent.add_skills(bland_altman_plot, change_from_baseline_plot, plot_endpoint_distribution, plot_correlation, severity_category_confusion_matrix, categorized_strip_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model, the commented out section is simple training. It works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instructions Training\n",
    "# # Correctly format the training document\n",
    "# training_docs = [\n",
    "#     \"For each pandasai skills function where any parameter is missing. Ask the user a clarifying question whether they want to use the default plot or specify a parameter. For example, if the use didn't specify to set the bySeverityCategory to True/ False, ask the user: Would you like to plot by severity category?\", \n",
    "#     \"Never return an input error, always ask the user to clarify function call parameters if mandatory ones are missing\"\n",
    "# ]\n",
    "\n",
    "# # Train the agent with the formatted documents\n",
    "# agent.train(docs=training_docs)\n",
    "\n",
    "\n",
    "# ## Q/A train\n",
    "# # Train for bland_altman_plot\n",
    "# query = \"Plot a bland altman plot comparing AHI for the devices\"\n",
    "# query2 = \"Generate the Bland-Altman plot for ahi using watchpat and psg devices\"\n",
    "# query3 = \"Generate the Bland-Altman plot for AHI for all severity categories\"\n",
    "# response = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "\n",
    "#     bland_altman_plot(df, endpoint1='AHI', endpoint2='AHI', device1='WatchPAT', device2='PSG', bySeverityCategory=False)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "# response3 = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "\n",
    "#     bland_altman_plot(df, endpoint1='AHI', endpoint2='AHI', device1='WatchPAT', device2='PSG', bySeverityCategory=True)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "\n",
    "# agent.train(queries=[query, query2, query3], codes=[response, response, response3])\n",
    "\n",
    "# #Train for change from baseline\n",
    "# query = \"Plot the change from baseline for AHI\"\n",
    "# query2 = \"Plot the AHI change from baseline for all devices\"\n",
    "# response = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     change_from_baseline_plot(df, endpoint='AHI', device=None)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "\n",
    "# query3 = \"Plot the PSG AHI change from baseline\"\n",
    "# response3 = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     change_from_baseline_plot(df, endpoint='AHI', device='PSG')\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "\n",
    "# agent.train(queries=[query, query2], codes=[response, response])\n",
    "\n",
    "# # Train for plotting endpoint distribution\n",
    "# query = \"Plot endpoint distribution for each device for each visit to have a general idea of device agreement\"\n",
    "# query2 = \"Plot AHI distribution by severity category\"\n",
    "# response = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     plot_endpoint_distribution(df, 'AHI', device1=None, device2=None, visit=None, bySeverityCategory = False)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "# response2 = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     plot_endpoint_distribution(df, 'AHI', device1=None, device2=None, visit=None, bySeverityCategory = True)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "# agent.train(queries=[query, query2], codes=[response, response2]) \n",
    "\n",
    "# # Train for ploting correlation between two endpoints or devices\n",
    "# query = \"Plot the correlation between the two devices for AHI\"\n",
    "# query2 = \"Plot AHI correlation comparing the devices\"\n",
    "# response = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     plot_correlation(df, endpoint1='AHI', endpoint2='AHI', device1='PSG', device2='WatchPAT', bySeverityCategory=False)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "# agent.train(queries=[query, query2], codes=[response, response]) \n",
    "\n",
    "# # Train for plotting severity category confusion matrix\n",
    "# query = \"Plot a confusion matrix visualizing how the severity category changes over time for AHI\"\n",
    "# query2 = \"Plot AHI severity confusion matrix\"\n",
    "# response = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     severity_category_confusion_matrix(df, endpoint, device=None, visit1='Screening', visit2=None)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "# agent.train(queries=[query, query2], codes=[response, response]) \n",
    "\n",
    "# # Train for creating a categorized strip plot\n",
    "# query = \"Create a categorized strip plot for AHI where PSG is the gold standard\"\n",
    "# response = \"\"\"\n",
    "# def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "#     df = dfs[0]\n",
    "    \n",
    "#     categorized_strip_plot(df, endpoint='AHI', gold_standard_device='PSG', visit=None)\n",
    "#     return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "# \"\"\"\n",
    "# agent.train(queries=[query], codes=[response]) \n",
    "\n",
    "# # The model will use the information provided in the training to generate a response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex QA Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions Training\n",
    "# Correctly format the training document\n",
    "training_docs = [\n",
    "    \"For each pandasai skills function where any parameter is missing. Ask the user a clarifying question whether they want to use the default plot or specify a parameter. For example, if the use didn't specify to set the bySeverityCategory to True/ False, ask the user: Would you like to plot by severity category?\", \n",
    "    \"Never return an input error or an empty graph, let the user know if their desired plot cannot be done using the custom function calls\", \n",
    "    \"always ask the user to clarify important function call parameters\"\n",
    "]\n",
    "\n",
    "# Train the agent with the formatted documents\n",
    "agent.train(docs=training_docs)\n",
    "\n",
    "## Q/A train\n",
    "# Train for bland_altman_plot\n",
    "query = \"Plot a bland altman plot comparing AHI for the devices\"\n",
    "query2 = \"Generate the Bland-Altman plot for ahi using watchpat and psg devices\"\n",
    "query3 = \"Generate the Bland-Altman plot for AHI for all severity categories\"\n",
    "response = \"\"\"\n",
    "def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "    df = dfs[0]\n",
    "\n",
    "    response = bland_altman_plot(df, endpoint1='AHI', endpoint2='AHI', device1='WatchPAT', device2='PSG', bySeverityCategory=False)\n",
    "    if response == \"error\":\n",
    "        return { \"type\": \"text\", \"value\": response}\n",
    "    return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "\"\"\"\n",
    "response3 = \"\"\"\n",
    "def analyze_data(dfs: list[pd.DataFrame]) -> dict:\n",
    "    df = dfs[0]\n",
    "\n",
    "    response = bland_altman_plot(df, endpoint1='AHI', endpoint2='AHI', device1='WatchPAT', device2='PSG', bySeverityCategory=True)\n",
    "    if response == \"error\":\n",
    "        return { \"type\": \"text\", \"value\": response}\n",
    "    return { \"type\": \"plot\", \"value\": \"temp_chart.png\"}\n",
    "\"\"\"\n",
    "\n",
    "# Attempt to train clarifying questions\n",
    "query4 = \"Plot a bland altman plot\"\n",
    "query5 = \"Tell me what I need to specify for you to plot a bland altman plot\"\n",
    "query6 = \"How do I plot a bland altman plot?\"\n",
    "query7 = \"What information do you need to create a bland altman plot?\"\n",
    "response4 = \"\"\"\n",
    "To plot a Bland-Altman plot, you need to specify the endpoints or devices to compare. Here are some examples:\n",
    "\n",
    "1. \"Plot a Bland-Altman plot comparing AHI and pAHI.\"\n",
    "2. \"Plot a Bland-Altman plot for AHI for the devices by severity category.\"\n",
    "\n",
    "Please provide the specific endpoints or categories you wish to compare.\n",
    "\"\"\"\n",
    "response5 = \"\"\"\n",
    "To plot a Bland-Altman plot, please provide the following details:\n",
    "\n",
    "1. **Endpoints to compare**: (e.g., AHI vs. pAHI)\n",
    "2. **Devices used for the measurement**: (e.g., WatchPAT and PSG)\n",
    "3. **Categorization by severity**: (e.g., mild, moderate, severe)\n",
    "\n",
    "For example, \"Plot a Bland-Altman plot comparing AHI for WatchPAT and PSG by severity category.\"\n",
    "\n",
    "\"\"\"\n",
    "response6 = \"\"\"\n",
    "To plot a Bland-Altman plot, please provide the following details:\n",
    "\n",
    "1. **Endpoints to compare**: (e.g., AHI vs. pAHI)\n",
    "2. **Devices used for the measurement**: (e.g., WatchPAT and PSG)\n",
    "3. **Categorization by severity**: (e.g., mild, moderate, severe)\n",
    "\n",
    "For example, \"Plot a Bland-Altman plot comparing AHI for WatchPAT and PSG by severity category.\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "response7 = \"\"\"\n",
    "To create a Bland-Altman plot, I need the following information:\n",
    "\n",
    "1. **Endpoints to compare**: What are the two measurements you want to compare? (e.g., AHI vs. pAHI)\n",
    "2. **Devices used for measurements**: Which devices were used for these measurements? (e.g., WatchPAT and PSG)\n",
    "3. **Severity categorization**: Do you want to categorize the data by severity? If yes, please provide the severity categories (e.g., mild, moderate, severe).\n",
    "\n",
    "For instance, you could request, \"Create a Bland-Altman plot comparing AHI and pAHI using WatchPAT and PSG, categorized by severity.\"\n",
    "\"\"\"\n",
    "agent.train(queries=[query, query2, query3, query4, query5, query6, query7], codes=[response, response, response3, response4, response5, response6, response7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test prompts which worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bland_altman_plot_test_prompts = [\"Plot a bland altman plot comparing AHI for both devices\"]\n",
    "change_from_baseline_plot_test_prompts = [\"Plot the AHI change from baseline for all devices\", \"Plot the AHI change from baseline\", \"Plot the AHI change from baseline for WatchPAT\"]\n",
    "endpoint_distribution_plot_test_prompts = [\"Plot the AHI distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_retry = 10000\n",
    "num_retries = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually handle query using pandasAI clarification questions api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def handle_query(query):\n",
    "    global num_retries\n",
    "    if num_retries < maximum_retry:\n",
    "        try:\n",
    "            response = agent.chat(query)\n",
    "        \n",
    "            # Try to parse the response as a JSON object if it's a string\n",
    "            try:\n",
    "                response_dict = json.loads(response)\n",
    "            except json.JSONDecodeError:\n",
    "                response_dict = {\"type\": \"text\", \"value\": response}\n",
    "\n",
    "        \n",
    "            if response[\"type\"] == \"error\":\n",
    "                num_retries += 1\n",
    "                questions = agent.clarification_questions(query)\n",
    "                if questions:\n",
    "                    print(questions[0])\n",
    "            else:\n",
    "                num_retries = 0\n",
    "                print(response)\n",
    "        except Exception as e:\n",
    "            num_retries += 1\n",
    "            questions = agent.clarification_questions(query)\n",
    "            print(questions[0])\n",
    "    else:\n",
    "        print(\"Maximum retries exceeded. Your prompt does not contain enough information or is out of scope for this chatbot. Please look at the examples of valid prompts provided.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"Plot a bland altman plot comparing AHI for both devices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.last_code_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for questions cannot be answered, you can ask for explanation\n",
    "explanation = agent.explain()\n",
    "\n",
    "print(\"The answer is\", response)\n",
    "print(\"The explanation is\", explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define langchain tools\n",
    "from langchain.agents import tool\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create prompt\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, but don't know current events\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create pandasAI tool\n",
    "pandasaiAgent = None\n",
    "df = None\n",
    "\n",
    "@tool\n",
    "def call_pandasai(prompt: str):\n",
    "    \"\"\"\n",
    "    Calls the pandasai agent with the clarified, prompt engineered, user prompt. The user prompt can now be parse for function parameters for these functions:\n",
    "    1. def bland_altman_plot(df, endpoint1, endpoint2, device1=None, device2=None, bySeverityCategory=False):\n",
    "    2. def change_from_baseline_plot(df, endpoint, device=None):\n",
    "    3. def plot_endpoint_distribution(df, endpoint, device1=None, device2=None, visit=None, bySeverityCategory=False):\n",
    "    4. def plot_correlation(df, endpoint1, endpoint2, device1=None, device2=None, bySeverityCategory=False):\n",
    "    5. def severity_category_confusion_matrix(df, endpoint, device=None, visit1='Screening', visit2=None):\n",
    "    6. def categorized_strip_plot(df, endpoint, gold_standard_device, visit=None):\n",
    "    \"\"\"\n",
    "\n",
    "    global pandasaiAgent, df\n",
    "    # Initialize PandasAI agent if not already initialized\n",
    "    if pandasaiAgent is not None:\n",
    "        pandasaiAgent.chat(prompt)\n",
    "\n",
    "@tool\n",
    "def get_standardized_analysis_ready_df(study_name):\n",
    "    \"\"\" \n",
    "    Retrieves the standardized analysis ready data frame and initializes the pandasai agent if it doesn't already exist.\n",
    "    Args:\n",
    "    The name of the study: str\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    global studyName, df, pandasaiAgent\n",
    "    studyName = study_name\n",
    "    \n",
    "    # Load DataFrame\n",
    "    df = pd.read_csv(\"./data/standardized_analysis_ready_df.csv\")\n",
    "    df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    \n",
    "    if pandasaiAgent is None:\n",
    "        pandasaiAgent = Agent(df)\n",
    "        \n",
    "    return pandasaiAgent\n",
    "    \n",
    "    \n",
    "tools = [call_pandasai, get_standardized_analysis_ready_df]\n",
    "\n",
    "# Bind tools to LLM\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "MEMORY_KEY = \"chat_history\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Your job is to get information from a user about what type of task they want to execute and execute it through function calls.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=MEMORY_KEY),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a list to track the chat history\n",
    "\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running, we now need to track the inputs and outputs as chat history\n",
    "\n",
    "input1 = \"Plot a bland altman plot comparing AHI for WatchPat and PSG. study name: gpif\"\n",
    "result = agent_executor.invoke({\"input\": input1, \"chat_history\": chat_history})\n",
    "chat_history.extend(\n",
    "    [\n",
    "        HumanMessage(content=input1),\n",
    "        AIMessage(content=result[\"output\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
